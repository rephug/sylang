Designing an Efficient sylang Prime Tokenizer for Gemma 3 and Qwen 3 Models

sylang Prime is a constructed language explicitly optimized for Large Language Models, achieving 55–60% fewer tokens than English for the same content. To maximize this efficiency (\>45% token reduction vs English) while preserving semantic clarity, we propose a tokenizer design tailored to sylang Prime’s morphology and vocabulary. The design supports both Gemma 3 (SentencePiece-based) and Qwen 3 (Hugging Face tokenizers) for interoperability. Below, we detail the plan across key areas:

1\. Tokenization Algorithms: Unigram vs BPE vs WordPiece vs Hybrid

We evaluate four subword tokenization strategies for sylang Prime, considering their impact on token compression and semantic alignment:

Unigram Language Model (ULM) – A probabilistic model (used by SentencePiece) that starts with a large pool of candidate subwords and prunes it down to an optimal vocabulary by maximizing corpus likelihood. Pros: Tends to produce stable, statistically optimal segments and can assign whole morphemes as tokens if beneficial. This often yields slight perplexity gains for some languages by accounting for token context probabilities. Cons: The final segmentation can be less intuitive, and Unigram may include redundant token variants. Effectiveness can vary by language; one study found Unigram outperformed BPE for Romance languages, while BPE worked better for Germanic languages. sylang Prime’s regular morphology likely leans toward consistent subword patterns, meaning ULM could model its affix combinations well.

Byte-Pair Encoding (BPE) – An iterative merging algorithm that greedily combines the most frequent character or subword pairs into larger tokens. Pros: Deterministic and fast to train, yielding a concise vocabulary that often maximizes compression. BPE is used in many LLMs (GPT-2, LLaMA) and would merge common sylang syllable sequences or root+affix pairs into single tokens. Cons: Pure BPE may ignore linguistic boundaries, potentially merging across morphemes in ways misaligned with meaning (e.g. splitting a root or affix). This misalignment can hinder model learning for morphologically rich languages, as token boundaries not matching morphemes make it harder to learn the language’s structure. For sylang Prime, a naive BPE might achieve high compression but risk segmenting in semantically odd places if not guided.

WordPiece – Similar to BPE, but uses a maximum likelihood criterion (rather than purely frequency) for merges and often starts from words as initial tokens (used in BERT). Pros: Yields a balanced vocabulary that avoids extremely rare merges, and tends to keep frequent word pieces intact. It can be configured to never merge certain prefixes or suffixes. Cons: Slightly more complex training; still largely frequency-driven and may break morphological structure if not constrained. In practice, WordPiece and BPE behave similarly for large corpora. Given sylang Prime’s constrained lexicon and regular affixes, WordPiece would need explicit rules to respect those affix boundaries.

Hybrid Morphology-Aware Approach – We propose a two-pass tokenization that combines linguistic rules with data-driven subwords. In pass 1, a morphological analyzer splits sylang Prime words at known affix boundaries (using either a reserved delimiter or metadata) so that tokens align with morphemes. In pass 2, a subword algorithm (e.g. BPE or Unigram) is applied without merging across those boundaries. This approach is akin to MorphBPE, a recent method that “prevents frequent merges from crossing morpheme boundaries while keeping the rest of BPE unchanged”. It maintains compatibility with standard BPE decoding while enforcing morphological integrity. Pros: Combines the strengths of linguistic segmentation and statistical compression, ensuring each token is a meaningful unit (root or affix or common combination) and maximizing information per token. sylang Prime’s design already emphasizes morpheme-level tokenization (each affix or root ideally a token), so this hybrid aligns naturally. Cons: Requires a morphological dictionary or rules, and a more complex training pipeline (two-phase). However, the payoff is significant: the sylang Prime team reports a custom tokenizer with morpheme-aligned tokens and a two-pass BPE achieved 55–60% token count reduction vs English and clear semantic segmentation.

Recommended Choice: Use a morphology-aware subword tokenizer. This could be implemented as Unigram or BPE, but with constraints to respect sylang Prime’s morpheme boundaries. Given SentencePiece’s flexibility, one option is training a Unigram model on pre-segmented text (so it treats affixes as separate tokens) and then exporting those vocab units to a HuggingFace BPE tokenizer. Alternatively, train a BPE with a custom algorithm (MorphBPE style) that forbids merges across affix boundaries. This ensures high semantic alignment (tokens map cleanly to meaningful roots/affixes) and retains maximal compression of frequently co-occurring morpheme sequences.

2\. Optimal Vocabulary Size for sylang Prime

Choosing the right vocabulary size is critical to balance compression and generalization. sylang Prime’s linguistic profile – roughly 300 core roots and \~30 productive affixes, plus systematic compounds – suggests that an 8k–16k token vocabulary is appropriate:

Baseline Vocabulary: The sylang Prime reference design uses ≈8,192 tokens as an optimal point. This size is large enough to include all roots, affixes, and many common whole words or root+affix combinations, yet small enough to ensure each token appears frequently in training. An 8k vocab leverages the language’s compactness; for comparison, English LLM tokenizers often use 32k–50k tokens, reflecting English’s larger and less regular word inventory.

Compression vs Coverage: A smaller vocab (e.g. 4k) would force more splits and could undermine sylang’s token advantage, while a much larger vocab (20k+) might encode entire phrases as single tokens but risks many rare tokens that the model sees infrequently. Our target is to maximize token compression (fewer tokens per sentence) without overfitting rare forms. Experiments can validate that \~8k tokens already yield \>45% fewer tokens than English text. If further compression is needed for specialized domains, we could expand toward 12k–16k to include domain-specific terms or very frequent multi-morpheme words.

Morphology-Driven Estimate: With \~30 affixes and \~300 roots forming the productive core, their combinations yield on the order of a few thousand common words (many roots can combine with several affixes). Including all single morphemes (roots and affixes ≈330) plus multi-morpheme tokens (common words, compounds) up to a certain frequency, an 8k vocab covers most combinations. Indeed, the design calls for vocab distribution across morphemes, full words, and subwords. We will empirically tune this: for example, evaluate 8k vs 12k vocab by training tokenizers and checking token count on a benchmark corpus. The final choice will prioritize the smallest vocab that still achieves the \>45% token reduction goal.

3\. Morphological Segmentation Strategies for Clarity

To ensure the tokenizer respects sylang Prime’s agglutinative morphology, we incorporate explicit segmentation strategies so that token boundaries align with linguistic boundaries:

Affix Boundary Markers: We introduce a special delimiter or marker in the training corpus to denote morpheme boundaries (e.g. a zero-width joiner or a character like \_). The sylang Prime design even allows a dedicated character to mark morpheme breaks. For tokenizer training, we can insert this marker between a root and each suffix/prefix. This guides the algorithm to treat morpheme segments as separate units. For example, an inflected word root-suffix1-suffix2 might be represented as root\_^suffix1\_^suffix2 in training data (using ^ as a placeholder boundary symbol). This way, BPE or Unigram will not merge across ^, ensuring tokens like root, suffix1, suffix2 remain intact or only fuse in allowed combinations.

Agglutination and Hyphenation Rules: If sylang Prime permits compound words (root-root compositions) without spaces, we enforce a rule to split compounds at the constituent boundary unless the compound itself is extremely common. This could be done via a hyphen or the same boundary marker. For instance, if sunrise in sylang is formed by sun+rise as one orthographic word, we ensure it’s segmented into sun and rise tokens (unless sunrise occurs so frequently that we decide to include a fused token). The language’s design favors clear boundaries (“regular agglutination with explicit morpheme breaks”), so in many cases a reader can identify morpheme junctures by alternating consonant/vowel patterns. We will leverage this by writing a simple pre-tokenizer that splits on known affix sequences or CV patterns that signal affix starts (e.g. many suffixes might start with a consonant and follow a vowel in the root).

No Merge Across Morphemes: During subword vocabulary construction, we apply a morphological constraint: any merge that would join characters from two different morphemes is disallowed. This is exactly the principle behind MorphBPE. Implementing this requires knowing morpheme boundaries in each word of the training corpus. We can get this from the constructed corpus (since sylang Prime’s corpus is likely annotated or easily segmentable by rules). By doing so, we ensure the tokenizer yields morphologically aligned tokens – e.g., a sylang word with root "karo" \+ suffix "-mi" (making an adjective) should tokenize as \["karo", "mi"\], not \["kar", "omi"\]. Research indicates that such morphological alignment in tokenization improves LLM performance for rich morphology languages, as it helps the model learn each meaningful unit clearly. In sylang Prime, which was explicitly designed for unambiguous composition, honoring those boundaries will preserve semantic clarity (each token \= one semantic piece).

Strict Prefix/Suffix Handling: We list all official sylang Prime affixes (tense markers, derivational suffixes, etc.) and ensure the tokenizer can isolate them. For example, suffixes like -t (past tense), -s (future), -p (perfective) should virtually always be separate token units or part of a larger token that still encapsulates a full morpheme (like a frequent combo of root+suffix). We may configure the tokenizer with a list of tokens that must appear as whole units. SentencePiece supports specifying substrings that should not be broken, and in BPE we can initialize the vocab with all affixes as initial tokens so they won’t get merged into others. This guarantees clarity: e.g. the model will see “-t” and know it denotes past tense uniformly across contexts.

In summary, our segmentation strategy uses affix-aware pre-tokenization (via markers or rules) so that the subword algorithm operates on linguistically meaningful chunks. This preserves the transparent morphology of sylang Prime in the tokenized representation, aligning with the language’s goal of zero ambiguity for machine parsing.

4\. Fusion Token Mining for High-Frequency Compounds

Beyond single morphemes, we aim to include “fusion tokens” representing frequently co-occurring sequences of morphemes (or even multi-word phrases) to further compress text. The process for mining and integrating such tokens is:

Frequency Analysis: After initial tokenizer training (with morphological constraints), we analyze the training corpus for common token sequences. This can be done by running the corpus through the preliminary tokenizer and collecting statistics on token n-grams. We’ll look for bigrams or trigrams of tokens that occur very often in sequence. Candidates include root+affix combinations (e.g. a particular derived word that appears extremely frequently) and compound words or collocations (e.g. a two-word phrase that’s idiomatic in sylang Prime). For instance, if “government” in sylang is formed by the root for “govern” plus abstract suffix -xa (govern-xa), and this word appears a lot, the pair \["govern", "xa"\] might be merged into a single token “governxa” in the vocab.

Statistical Thresholds: We define criteria such as: any token bigram that occurs, say, \>N times per million tokens, or contributes more than X% to corpus frequency, is a fusion candidate. We must balance informativeness vs frequency – a compound token is most useful if it significantly reduces token count and appears often enough to justify occupying a slot in the vocabulary. The target \>45% token reduction suggests we aggressively capture frequent multi-morpheme units. sylang Prime’s designers note an “optimal distribution across morphemes, words, and subword components” in the 8192 vocabulary, implying that many high-frequency full words are directly included as single tokens.

Manual Linguistic Input: We will also incorporate known common constructions. For example, if sylang Prime frequently forms polite verb forms or fixed expressions by combining certain affixes, those could be pre-emptively added. Domain-specific terms that consist of multiple roots or affixes (from technical or specialized vocabulary) can be considered for fusion tokens if they recur often in domain corpora (point 8 in this proposal covers updating for new terms).

Integration into Tokenizer: Once we identify a list of candidate fused tokens, we integrate them by adding them to the vocabulary and retraining or updating merges. In a BPE framework, this is naturally handled by the merging process – the most frequent pairs will be merged first anyway. In a Unigram framework, we can manually add these as seed tokens with high prior probability or simply include them in the input corpus as unbroken units so the model learns them. Another approach is to run a second pass of BPE merge on top of the morphologically segmented corpus focusing only on allowed merges (root+affix or word+word merges). In fact, our two-pass tokenizer design (Section 1\) inherently does this: first pass ensures only valid segments, second pass performs BPE merges which will fuse the most frequent adjacent segments. This will automatically yield common combinations as single tokens. We will verify that the resulting vocab contains expected frequent compounds.

Example Fusion: Suppose sylang Prime has an extremely common phrase equivalent to “thank you” composed of two words. If that bigram occurs with very high frequency, we might include a single token for “thankyou” to save a token and speed up processing. Similarly, if a particular root+derivational affix combination forms a concept that appears constantly (like education \= educate \+ -tion), it may merit a dedicated token.

By mining and adding these fusion tokens, we ensure the tokenizer maximizes token compression on real data. The result is that even multi-morpheme words often become one token if they are common, pushing sylang Prime’s efficiency well above the 45% reduction target. Importantly, we only fuse in ways that do not obscure internal semantics – typically within a compound or between a root and its affix (never in the middle of a root or affix). This preserves interpretability while reducing token count.

5\. Preprocessing and Normalization Rules

Consistent preprocessing of text before tokenization will further improve tokenizer reliability. We establish rules to normalize sylang Prime input so that the tokenizer behaves consistently:

Case and Unicode Normalization: sylang Prime uses a simple ASCII orthography, so we enforce lowercase throughout (assuming the language is caseless or uses one case). Any user text is lowercased and stripped of diacritics (not applicable here) and normalized to NFC form. Standard Unicode normalization ensures no unexpected byte sequences. Since only 21 letters are valid, any out-of-alphabet characters (e.g. foreign names or symbols) can be handled by a fallback (see below).

Whitespace and Punctuation: We preserve spaces as word delimiters (unless sylang explicitly allows concatenation without spaces). SentencePiece can treat whitespace as a normal character (the "▁" symbol for word start) – we will use that default to maintain word boundaries. For HuggingFace tokenizers, we’ll use a basic pre-tokenizer that splits on whitespace and punctuation (so punctuation marks become separate tokens or are tagged as special tokens if needed). All punctuation in sylang should be limited (commas, periods, etc.), and we can normalize quotes, dashes, etc., to a single standard form. E.g., convert any fancy Unicode quote to a simple ". This prevents tokenization differences caused by multiple symbols for the same concept.

Affix Joiner Handling: If we use an explicit morpheme boundary marker (like \_ or ^ in the training data), we need a rule for the actual inference-time text. One approach is to not actually include the marker in real text, but only in training data to guide the tokenizer. Alternatively, if sylang Prime writing system chooses to visually include a marker (perhaps a special hyphen or apostrophe to mark certain affixes), we will keep it consistent. For instance, if all derivational affixes in text are written with a preceding ' (just hypothetically), we ensure the tokenizer always splits at that '. In practice, we likely keep sylang text continuous and rely on the trained model to know where to split – thus, the marker is a training artifact. Post-training, we would remove it from the final vocabulary (it would not appear as an actual token for inference) but its effect is baked into the learned merges.

Zero-Width Joiner Technique: A trick to ensure consistent breaking at boundaries is inserting a zero-width joiner character between morphemes in the corpus. This character has no visible representation but can block merges. We need to ensure the tokenizer and model are aware of it. More simply, we might avoid this and rely on explicit segmentation as earlier described.

Numeric and Special Tokens: If sylang Prime uses its own numeric system or just borrows digits 0-9, we define how numbers are tokenized (e.g., each digit as a token vs the entire number as one token). Likely, we treat numbers as separate tokens or use a single numeric token if the language has a compact numeral word. We also include any needed special tokens: e.g., an end-of-sentence marker or metadata tags if required by Gemma/Qwen.

Consistency in Rare Cases: For any constructs that could be written in multiple forms, we enforce one. For example, if an affix could be optionally separated by a hyphen for clarity, we decide one way (likely no hyphen in standard text, relying on the language’s inherent boundary clarity). All training data should follow the same convention. If the corpus includes some English or other language text (in translations), we might either translate those to sylang or mark them as foreign (the tokenizer could handle them character-by-character or via a separate “\<foreign\>” token if such cases are expected).

Overall, the preprocessing ensures that the input text is clean, lowercase, and segmented in a uniform way before tokenization. This avoids the tokenizer seeing out-of-vocabulary character sequences or inconsistent use of affixes. By normalizing these aspects, we improve the tokenizer’s consistency and the model’s understanding (garbage in, garbage out\!). The sylang Prime corpus creation guidelines already emphasize careful curation and normalization, so our tokenizer will build on that foundation.

6\. Benchmarking Tokenizer Efficiency and Alignment

We will rigorously benchmark the proposed tokenizer to verify its advantages. Two key baselines for comparison are:

English BPE Tokenizer (GPT-2 or LLaMA) – We will take a standard English subword tokenizer (e.g. GPT-2’s 50k BPE or LLaMA’s 32k BPE) and run it on a set of parallel texts: original English vs the equivalent sylang Prime translation. This measures the token count difference. We expect \>45% fewer tokens with sylang Prime. For example, if an English paragraph is 100 tokens, the same content in sylang Prime might be \~50–60 tokens with our tokenizer. This directly confirms the compression gains of the language+tokenizer. We’ll cite aggregate stats: e.g., average token length per sentence in sylang vs English. Additionally, we compare to the theoretical maximum reduction: sylang Prime’s docs claim 55–60% fewer tokens than English, so our actual tokenizer should approach that. If we fall short, we’ll analyze if the tokenizer is splitting too much (perhaps vocabulary too small or segmentation too conservative) and adjust accordingly.

Standard sylang Prime BPE – We also compare against a naïve BPE tokenizer trained on sylang Prime without special morphological handling. This baseline is essentially “what if we just ran SentencePiece BPE on the corpus with a similar vocab size, but no linguistic guidance”. In this scenario, we anticipate that our custom tokenizer will show:

Better Morphological Alignment: The naive BPE might produce odd splits (e.g., breaking an affix or combining pieces of adjacent words if spacing isn't strict). Our tokenizer should demonstrate that tokens correspond cleanly to morphemes. We can quantify this by measuring a morphological alignment score (similar to MorphScore or the percentage of tokens that are complete morphemes). We expect near 100% alignment in our tokenizer versus significantly lower in a naive one.

Comparable or Improved Compression: A naive BPE might actually pack slightly more content per token (since it has no constraints, it might merge more freely), but any marginal token count gain could come at the cost of confusing splits. We predict that with an 8192 vocab, both approaches compress well, but our approach should still be within a few percent of naive BPE’s token count. If there is a compression penalty for respecting morphemes, it should be small, because sylang’s affixes are short (often one or two letters) and frequently occurring, meaning BPE would have merged them anyway. If anything, our method might even outperform naive BPE on compression for long words, because by treating whole frequent words as tokens we capture multi-morpheme units intelligently, whereas naive BPE might stick to very frequent subword pieces and not learn some longer words as single tokens. We will confirm this by comparing average tokens per word on a test set.

Impact on Model Performance: While full LLM training is expensive, we reference literature that fewer tokens can improve LLM performance up to a point (more content per context window), but only if those tokens still represent meaningful units. We expect our morphologically aligned tokenizer to improve model’s perplexity and downstream accuracy compared to a misaligned one. If feasible, we will train small LLM prototypes on each tokenizer and evaluate perplexity or reasoning tasks to demonstrate the semantic alignment benefit. Prior research notes that simply minimizing token count isn’t everything – **“tokenization is more than compression”** – so we focus on meaningful compression. Our benchmarks will highlight that we achieved both: large token count reduction and tokens that correspond to intuitive language units (leading to better learning).

Examples and Qualitative Check: We’ll present example tokenizations side-by-side. For an English sentence, show GPT-2 BPE tokens vs sylang Prime tokens (dramatically fewer). For a sylang Prime complex word, show naive vs custom tokenizer segmentation. For instance, given a sylang word meaning “unhappiness” composed of happy (root) \+ un- (negation prefix) \+ -ness (noun suffix), a naive tokenizer might split it as \["ha", "ppy", "un", "ness"\] (hypothetically), whereas our tokenizer would yield \["un", "happy", "ness"\] – 3 tokens, each a morpheme. Such qualitative examples will reinforce the quantitative metrics.

These benchmarks will be documented to validate that the recommended tokenizer meets or exceeds the design goals: \>45% token reduction vs English, morphologically coherent tokens, and effective handling of sylang Prime’s structure.

7\. Tokenizer Training Implementation (SentencePiece & HF)

To deploy the tokenizer for both Gemma 3 and Qwen 3, we outline training procedures and configuration for SentencePiece (SP) and Hugging Face (HF) tokenizers:

SentencePiece Training (for Gemma 3):

Algorithm & Corpus: We will use SentencePiece’s Python API or command-line to train on the full sylang Prime corpus (which should be at least 1M tokens, ensuring diverse coverage). We choose the --model\_type depending on our earlier decision (likely unigram or a guided bpe). Given our hybrid approach, one method is to perform morphological pre-segmentation on the corpus (insert boundary markers or spaces at morpheme breaks) and then run SentencePiece in bpe mode. Alternatively, we can let SentencePiece run in unigram mode with the segmented text. The vocabulary size --vocab\_size=8192 (or adjusted value after experiments) will be set. We also include --character\_coverage=1.0 since our character set is closed (21 letters and few punctuation), meaning we don’t need fallback bytes.

Special Configs: We can utilize SentencePiece’s --hard\_vocab\_limit=false in case we want to manually add some tokens later, and --unk\_piece=\<unk\> etc., to define unknown token. We also might use the --user\_defined\_symbols option to reserve tokens for things like \<sep\> or any special syntax markers sylang uses (though sylang might not need many such tokens due to its own explicit markers). If we inserted a boundary marker like ^, we ensure SentencePiece treats it as a normal character (so it can appear in tokens but ideally it should appear only at boundaries). If we want to avoid ^ in final vocab, we might remove it after training – one approach is to use a placeholder that SP will never merge into other tokens (for instance, turning ^ into a token by itself). SP also has --split\_by\_whitespace=true by default; we will keep that so it doesn’t fuse across spaces, since spaces denote separate words in sylang.

Output: SentencePiece will produce a model file (sylangPrime.model) and vocabulary (sylangPrime.vocab). We will verify that all expected tokens (affixes, roots, common words) appear. If some crucial affixes are missing (could happen if they were rare in corpus), we may augment the training data or use --required\_chars / --vocab\_file initialization to force include them. The resulting SentencePiece model will be used directly by Gemma 3’s training pipeline (Gemma can integrate SP tokenization easily). The training script will be documented, e.g.:

spm\_train --input=sylang\_corpus.txt --model\_prefix=sylangPrime \\  
          --vocab\_size=8192 --model\_type=bpe \\  
          --user\_defined\_symbols=\<s\>,\</s\> --unk\_piece=\<unk\> \\  
          --max\_sentence\_length=10000

(Parameters may be adjusted; model\_type=unigram if we choose Unigram LM.)

Validation: We’ll test the SP tokenizer on sample sentences and ensure it segments as intended (especially at morpheme boundaries).

Hugging Face Tokenizer (for Qwen 3):

Conversion vs Training: To maintain identical tokenization between Gemma and Qwen, the simplest path is to convert the SentencePiece model into a HuggingFace-compatible tokenizer. HuggingFace’s transformers library can load SentencePiece models directly (for example, via XLMRobertaTokenizerFast.from\_pretrained with an SP model, or using SentencePieceProcessor). However, if Qwen requires a native HF tokenizers JSON, we can generate that. There are tools (like sentencepiecePB2huggingface) to convert an SP model to a HuggingFace fast tokenizer. This ensures both models use the exact same vocabulary and segmentation logic.

Direct Training Option: Alternatively, we can train a HuggingFace tokenizer from scratch using the 🤗 tokenizers library. We would replicate the approach: use tokenizers.Trainer with BPE, enforce our special tokens and pre-tokenization. For example:

from tokenizers import Tokenizer, models, trainers, pre\_tokenizers  
tokenizer \= Tokenizer(models.BPE())  
tokenizer.pre\_tokenizer \= pre\_tokenizers.Sequence(\[  
    pre\_tokenizers.Whitespace(),   
    CustomMorphPreTokenizer()  # our own that inserts boundaries or splits  
\])  
trainer \= trainers.BpeTrainer(vocab\_size=8192, special\_tokens=\["\<unk\>","\<s\>","\</s\>"\])  
tokenizer.train(files=\["sylang\_corpus.txt"\], trainer=trainer)  
tokenizer.save("sylangPrime-bpe.json")

In this snippet, CustomMorphPreTokenizer would be a routine we implement to mimic the morphological splitting (we can also achieve it by simply ensuring the corpus file already has spaces at morpheme boundaries as input to HF training). The HF BPE trainer will then treat each space-separated unit as a token candidate, effectively respecting our morpheme units similarly to the SP approach. We’d confirm the resulting vocab matches the SP one or is very close.

Tokenizer Configuration: The HF tokenizer JSON (or equivalent) will include the merges (for BPE) and the vocabulary mapping. We need to ensure special tokens are handled (the start, end, unk tokens). Qwen 3 might have specific requirements for the tokenizer class (e.g., using BertTokenizer vs GPT2Tokenizer style). Given sylang is not whitespace-agnostic like Chinese, we’ll likely end up with a standard WordPiece-like tokenizer (with ## prefix for continuation if we were doing WordPiece). But since we prefer explicit segmentation, we might not need any continuation marker – every subword corresponds to a whole morpheme or part that was delineated by boundaries.

Testing and Integration: We’ll test that the HF tokenizer produces the same tokenization as the SP one on a sample of sentences. This can be automated by comparing outputs. Once verified, Qwen 3’s model can use this tokenizer (e.g., via AutoTokenizer.from\_pretrained("sylangPrime-bpe.json")). Documentation will be provided on how to plug this in.

Ensuring Interoperability: Because Gemma 3 and Qwen 3 will share the tokenizer logic, any updates (like adding tokens later) should be applied to both SP and HF versions. During training of the LLMs, the text is tokenized through these tokenizers – having consistent behavior is crucial if we want to cross-compare or ensemble the models later.

Training Scripts: We will provide a script train\_sylang\_tokenizer.py that wraps the above logic. It will accept flags to choose SentencePiece or HF, and output the appropriate files. The script will incorporate our segmentation preprocessing (either by reading an already segmented corpus file or by applying a segmentation function on the fly). Configuration for the script includes parameters like vocab size, whether to use Unigram or BPE, and paths to input data. By making this reproducible, future re-training (if sylang Prime evolves) will be straightforward.

In summary, the training process produces two equivalent tokenizers: one in SentencePiece format for Gemma, and one in HuggingFace format for Qwen. Both implement the same subword rules (morpheme-aware, \~8k vocab). This dual implementation meets the technical requirements of each LLM framework without divergence in how text is tokenized.

8\. Adapting to Evolving Lexicon and Affixes

Languages grow over time – even a constructed language like sylang Prime might gain new vocabulary (new technical terms, names, or a deliberate expansion of roots/affixes). We plan for maintaining and updating the tokenizer as the language evolves:

New Root Words: If new core root words are added to sylang Prime (say a batch of 100 new STEM terms or slang expressions), the tokenizer should accommodate them ideally as single tokens (for efficiency and because they carry distinct meaning). If the new roots are composed of existing syllables, the tokenizer will by default break them into known subwords, which the model can handle but possibly with some loss of efficiency. To integrate them more fully, we have options:

Periodically retrain or fine-tune the tokenizer on an updated corpus that includes the new vocabulary. For example, every year or every major language update, we can rebuild the SentencePiece model with an expanded vocab size if needed. However, retraining can change token assignments for existing text, which might require careful handling if models are already trained.

Use reserved vocabulary slots: We could initially train with a slightly smaller effective vocab (say 7900 out of 8192\) and keep some IDs free. Then assign new words to those IDs later. This is tricky with subword models but not impossible – it requires the new tokens not to conflict with old ones. Alternatively, we leverage the model’s \<unk\> mechanism: initially, new words would be “unknown” (broken into chars or sub-pieces), but we can update the tokenizer’s vocab by adding the whole new word as a token mapping to a new ID. HuggingFace tokenizers allow adding tokens post-training (which maps them to new IDs beyond the original vocab). Gemma’s SentencePiece can’t easily add to an existing model, so we’d have to distribute a new SentencePiece model for the update.

In practice, a safe approach is to train the initial tokenizer to include a broad range of vocabulary (perhaps using pseudo-data or anticipated words) to minimize the need for immediate updates. sylang Prime’s systematic word formation means many new concepts are just combinations of existing morphemes, which the tokenizer can already handle.

New Affixes or Grammatical Particles: If the language designers introduce a new affix (e.g. a new case marker or aspect marker), this is critical to handle. Such an affix should be added as a token in the vocabulary to maintain our morphological clarity principle. Since the affix is likely a short sequence, if it appears in text without the tokenizer knowing it, the tokenizer might split it into characters or merge it with a root incorrectly. To prevent confusion:

We will update the morphological segmentation rules to recognize the new affix and always split it out.

Then, at tokenizer level, add this affix sequence as a token. If we cannot re-train immediately, a workaround could be to map the affix to the \<unk\> token temporarily, but that’s not ideal. Preferably, we do a tokenizer update release whenever a new affix is added.

Another idea: design the set of affixes to be closed and known upfront (which it largely is – e.g., 20–30 derivational affixes, and a handful of grammatical markers). We likely know them all from the start, so truly new affixes would be rare. The language engineering doc specifies no irregular forms and a fixed affix set, so our tokenizer can simply include all of them from day one.

Dynamic Compound Growth: As sylang usage grows, certain compound words or phrases might become very popular (e.g. a meme or a new technical term consisting of two words). The tokenizer could be periodically enhanced with new fusion tokens for these, using the same mining approach described in section 4 on newer corpora. If we find that users frequently use a particular 3-word sequence, adding it as a single token could be beneficial. This is analogous to how some tokenizers for models like GPT-3 were updated to include popular new terms or names that emerged after the model’s initial training.

Backward Compatibility: When updating the tokenizer, we must consider models already trained with an older tokenizer. Gemma 3 and Qwen 3 models have embeddings tied to token IDs. Adding new tokens (if adding to the end of vocab) can be handled by allocating new embedding vectors (initialized, say, to small random values or an average of others) and possibly fine-tuning the model a bit to adjust. Removing or changing tokens is much harder without retraining the model from scratch. Therefore, any evolution should be mostly additive. This means we prefer to add new tokens for new vocabulary rather than altering existing ones. If the vocab size was fixed, we might choose to increase it slightly (which means technically a new model embedding matrix if we fully retrain). For Qwen’s HF tokenizer, adding tokens is straightforward (e.g. tokenizer.add\_tokens(\["newword"\])). For Gemma’s SP, we would distribute a new .model file.

Automation and Monitoring: We will set up a pipeline to monitor incoming sylang Prime text (from user queries or new documents) to catch OOV (out-of-vocabulary) rates. If certain sequences consistently become OOV or are split into many tokens, that flags a need to update the tokenizer. For example, if a new proper noun “Zephylon” appears frequently and is not in vocab, we can decide to add it as a token (especially if it’s important in context). The monitoring could be as simple as logging unknown token frequency or average token count per sentence over time.

Continuous Learning Mode (Optional): As an experimental approach, we could incorporate an adaptive tokenizer strategy where the tokenizer is refined based on model feedback. Recent research suggests adjusting token vocab based on an LLM’s loss contribution per token (dropping tokens that the model predicts poorly). While we may not implement this initially, it’s a potential future direction: as Gemma/Qwen are trained on sylang, we could identify if some tokens are under-utilized or if splitting them differently would improve modeling. An adaptive process could then refine the tokenizer and possibly continue training. This ensures the tokenizer evolves in tandem with model understanding. However, this is complex and requires careful coordination between tokenizer and model retraining, so it’s a long-term consideration.

Summary of Adaptation: We design the sylang Prime tokenizer to be robust to growth. In practice, the constructed nature of sylang Prime means changes are planned and centralized (unlike natural languages that evolve spontaneously). We will likely issue tokenizer updates alongside language updates. Documentation will guide users on updating their Tokenizer files and the model to handle new vocabulary. By keeping tokens aligned to morphemes and reserving space for new terms, we ensure the tokenizer remains efficient and semantically precise as sylang Prime’s lexicon expands.

—

References & Best Practices: Our approach is informed by recent findings in LLM tokenization. For instance, researchers emphasize that morphologically aligned tokenization (splitting words at morpheme boundaries) is crucial for complex languages. The MorphBPE algorithm demonstrates how incorporating linguistic boundaries yields better consistency without sacrificing compatibility. We also heed that pure compression isn’t everything – our design optimizes token count and token meaning. The sylang Prime language engineering documents provided the blueprint for an 8192-size vocab with two-pass tokenization and achieved remarkable efficiency, which our proposal builds upon. By combining these best practices with a careful implementation for both SentencePiece and HuggingFace ecosystems, we can deliver a tokenizer that maximizes compression (\>45% fewer tokens than English), maintains clear semantic units for the LLM, and flexibly adapts to the future of the sylang Prime language.